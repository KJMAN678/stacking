{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "from sklearn.datasets import load_boston\r\n",
    "\r\n",
    "from sklearn.model_selection import KFold\r\n",
    "\r\n",
    "from sklearn.metrics import mean_squared_error\r\n",
    "\r\n",
    "import lightgbm as lgb\r\n",
    "import xgboost as xgb\r\n",
    "from catboost import Pool\r\n",
    "from catboost import CatBoost"
   ],
   "outputs": [],
   "metadata": {
    "id": "hPpYJxggr6SD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## データセット"
   ],
   "metadata": {
    "id": "s-MChoRfvUEX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "boston = load_boston()\r\n",
    "\r\n",
    "df = pd.concat([pd.DataFrame(boston.data, columns=boston.feature_names),\r\n",
    "           pd.DataFrame(boston.target, columns=[\"target\"])], \r\n",
    "          axis=1)"
   ],
   "outputs": [],
   "metadata": {
    "id": "oZ1SR4tErqPD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XGBoost"
   ],
   "metadata": {
    "id": "g0-wnyTbsozZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def xgb_train_cv(X_train_cv, y_train_cv, X_eval_cv, y_eval_cv, loop_counts):\r\n",
    "    # データを格納する\r\n",
    "    # 学習用\r\n",
    "    xgb_train = xgb.DMatrix(X_train_cv, label=y_train_cv)\r\n",
    "    # 検証用\r\n",
    "    xgb_eval = xgb.DMatrix(X_eval_cv, label=y_eval_cv)\r\n",
    "    # テスト用\r\n",
    "    #xgb_test = xgb.DMatrix(X_test, label=y_test)\r\n",
    "\r\n",
    "    xgb_params = {\r\n",
    "        'objective': 'reg:squarederror',  # 回帰\r\n",
    "        'eval_metric': 'rmse'           # 学習用の指標 (RMSE)\r\n",
    "    }\r\n",
    "\r\n",
    "    # 学習\r\n",
    "    evals = [(xgb_train, 'train'), (xgb_eval, 'eval')] # 学習に用いる検証用データ\r\n",
    "    evaluation_results = {}                            # 学習の経過を保存する箱\r\n",
    "    bst = xgb.train(xgb_params,                        # 上記で設定したパラメータ\r\n",
    "                    xgb_train,                         # 使用するデータセット\r\n",
    "                    num_boost_round=200,               # 学習の回数\r\n",
    "                    early_stopping_rounds=10,          # アーリーストッピング\r\n",
    "                    evals=evals,                       # 学習経過で表示する名称\r\n",
    "                    evals_result=evaluation_results,   # 上記で設定した検証用データ\r\n",
    "                    verbose_eval=0                     # 学習の経過の表示(非表示)\r\n",
    "                    )\r\n",
    "    \r\n",
    "    # 検証用データで予測\r\n",
    "    y_pred = bst.predict(xgb_eval, ntree_limit=bst.best_ntree_limit)\r\n",
    "\r\n",
    "    print('Trial: ' + str(loop_counts))\r\n",
    "    \r\n",
    "    # RMSEの評価\r\n",
    "    rmse = mean_squared_error(y_eval_cv, y_pred, squared=True)\r\n",
    "    print('XGBoost Validation:', rmse)\r\n",
    "    \r\n",
    "    return(bst, rmse, y_pred)"
   ],
   "outputs": [],
   "metadata": {
    "id": "i26kJrOWsHbI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LightGBM"
   ],
   "metadata": {
    "id": "PyZZmp1Nsrkd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def lgbm_train_cv(X_train_cv, y_train_cv, X_eval_cv, y_eval_cv):\r\n",
    "    # データを格納する\r\n",
    "    # 学習用\r\n",
    "    lgb_train = lgb.Dataset(X_train_cv, y_train_cv,\r\n",
    "                            free_raw_data=False)\r\n",
    "    # 検証用\r\n",
    "    lgb_eval = lgb.Dataset(X_eval_cv, y_eval_cv, reference=lgb_train,\r\n",
    "                           free_raw_data=False)\r\n",
    "    \r\n",
    "    # パラメータを設定\r\n",
    "    params = {'task': 'train',                # レーニング ⇔　予測predict\r\n",
    "              'boosting_type': 'gbdt',        # 勾配ブースティング\r\n",
    "              'objective': 'regression',      # 目的関数：多値分類、マルチクラス分類\r\n",
    "              'metric': 'rmse',      # 検証用データセットで、分類モデルの性能を測る指標\r\n",
    "              'learning_rate': 0.1,           # 学習率（初期値0.1）\r\n",
    "              'num_leaves': 23,               # 決定木の複雑度を調整（初期値31）\r\n",
    "              'min_data_in_leaf': 1,          # データの最小数（初期値20）\r\n",
    "             }\r\n",
    "\r\n",
    "    # 学習\r\n",
    "    evaluation_results = {}                                # 学習の経過を保存する箱\r\n",
    "    model = lgb.train(params,                              # 上記で設定したパラメータ\r\n",
    "                      lgb_train,                           # 使用するデータセット\r\n",
    "                      num_boost_round=200,                 # 学習の回数\r\n",
    "                      valid_names=['train', 'valid'],      # 学習経過で表示する名称\r\n",
    "                      valid_sets=[lgb_train, lgb_eval],    # モデルの検証に使用するデータセット\r\n",
    "                      evals_result=evaluation_results,     # 学習の経過を保存\r\n",
    "                      early_stopping_rounds=10,            # アーリーストッピングの回数\r\n",
    "                      verbose_eval=0                      # 学習の経過を表示する刻み（非表示）\r\n",
    "    )\r\n",
    "\r\n",
    "    # 検証用データで予測\r\n",
    "    y_pred = model.predict(X_eval_cv, num_iteration=model.best_iteration)\r\n",
    "    \r\n",
    "    # RMSEの評価\r\n",
    "    rmse = mean_squared_error(y_eval_cv, y_pred, squared=True)\r\n",
    "    print('LightGBM Validation:', rmse)\r\n",
    "    \r\n",
    "    return(model, rmse, y_pred)"
   ],
   "outputs": [],
   "metadata": {
    "id": "_1YsnEUmsvSY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CatBoost"
   ],
   "metadata": {
    "id": "rm1QXS7osyiA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def catboost_train_cv(X_train_cv, y_train_cv, X_eval_cv, y_eval_cv):\r\n",
    "    # データを格納する\r\n",
    "    # 学習用\r\n",
    "    CatBoost_train = Pool(X_train_cv, label=y_train_cv)\r\n",
    "    # 検証用\r\n",
    "    CatBoost_eval = Pool(X_eval_cv, label=y_eval_cv)\r\n",
    "\r\n",
    "    # パラメータを設定\r\n",
    "    params = {        \r\n",
    "        'loss_function': 'RMSE',          # 多値分類問題\r\n",
    "        'num_boost_round': 1000,          # 学習の回数\r\n",
    "        'early_stopping_rounds': 10       # アーリーストッピングの回数\r\n",
    "    }\r\n",
    "\r\n",
    "    # 学習\r\n",
    "    catb = CatBoost(params)\r\n",
    "    catb.fit(CatBoost_train, eval_set=[CatBoost_eval], verbose=False)\r\n",
    "\r\n",
    "    # 検証用データで予測\r\n",
    "    y_pred = catb.predict(X_eval_cv)\r\n",
    "    \r\n",
    "    # RMSEの評価\r\n",
    "    rmse = mean_squared_error(y_eval_cv, y_pred, squared=True)\r\n",
    "    print('CatBoost Validation:', rmse)\r\n",
    "    \r\n",
    "    return(catb, rmse, y_pred)"
   ],
   "outputs": [],
   "metadata": {
    "id": "DBJpNy68sx79"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 学習"
   ],
   "metadata": {
    "id": "m7DAKi4Ns4XY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# 各5つのモデルを保存するリストの初期化\r\n",
    "xgb_models = []\r\n",
    "lgbm_models = []\r\n",
    "catb_models = []\r\n",
    "\r\n",
    "# 各5つのモデルの正答率を保存するリストの初期化\r\n",
    "xgb_validations = []\r\n",
    "lgbm_validations = []\r\n",
    "catb_validations = []\r\n",
    "\r\n",
    "# 学習のカウンター\r\n",
    "loop_counts = 1\r\n",
    "\r\n",
    "# 各クラスの確率（3モデル*5seed*３クラス）\r\n",
    "first_reg = pd.DataFrame(np.zeros((len(df), 3*5*3)))\r\n",
    "\r\n",
    "\r\n",
    "for seed_no in range(5): \r\n",
    "        \r\n",
    "    # 学習データの数だけの数列（0行から最終行まで連番）\r\n",
    "    row_no_list = list(range(len(df)))\r\n",
    "\r\n",
    "    # KFoldクラスをインスタンス化（これを使って5分割する）\r\n",
    "    K_fold =  KFold(n_splits=5, shuffle=True, random_state=42)\r\n",
    "    \r\n",
    "    # KFoldクラスで分割した回数だけ実行（ここでは5回）\r\n",
    "    for train_cv_no, eval_cv_no in K_fold.split(row_no_list, df[\"target\"]):\r\n",
    "        # ilocで取り出す行を指定\r\n",
    "        X_train_cv = df.drop([\"target\"], axis=1).iloc[train_cv_no, :]\r\n",
    "        y_train_cv = df[\"target\"].iloc[train_cv_no]\r\n",
    "        X_eval_cv  = df.drop([\"target\"], axis=1).iloc[eval_cv_no, :]\r\n",
    "        y_eval_cv  = df[\"target\"].iloc[eval_cv_no]\r\n",
    "        \r\n",
    "        # XGBoostの訓練を実行\r\n",
    "        bst, bst_validation, xgb_reg = xgb_train_cv(X_train_cv, y_train_cv,\r\n",
    "                                                   X_eval_cv, y_eval_cv, \r\n",
    "                                                   loop_counts)\r\n",
    "        # LIghtGBMの訓練を実行\r\n",
    "        model, model_validation, lgbm_reg = lgbm_train_cv(X_train_cv, y_train_cv, \r\n",
    "                                                         X_eval_cv, y_eval_cv)\r\n",
    "        # CatBoostの訓練を実行\r\n",
    "        catb, catb_validation, catb_reg = catboost_train_cv(X_train_cv, y_train_cv,\r\n",
    "                                                           X_eval_cv, y_eval_cv)\r\n",
    "        # 実行回数のカウント\r\n",
    "        loop_counts += 1\r\n",
    "        \r\n",
    "        # 学習が終わったモデルをリストに入れておく\r\n",
    "        xgb_models.append(bst) \r\n",
    "        lgbm_models.append(model) \r\n",
    "        catb_models.append(catb) \r\n",
    "        \r\n",
    "        # 学習が終わったモデルの正答率をリストに入れておく\r\n",
    "        xgb_validations.append(bst_validation) \r\n",
    "        lgbm_validations.append(model_validation) \r\n",
    "        catb_validations.append(catb_validation) \r\n",
    "        \r\n",
    "        # 検証データの各クラスの回帰\r\n",
    "        for i in range(3):\r\n",
    "            first_reg.iloc[eval_cv_no, (seed_no * 3) + i] = xgb_reg[i]\r\n",
    "            first_reg.iloc[eval_cv_no, (seed_no * 3) + 15 + i] = lgbm_reg[i]\r\n",
    "            first_reg.iloc[eval_cv_no, (seed_no * 3) + 30 + i] = catb_reg[i]\r\n",
    "            \r\n",
    "first_reg.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Trial: 1\n",
      "XGBoost Validation: 6.55995747666388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000517 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1030\n",
      "[LightGBM] [Info] Number of data points in the train set: 404, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 22.796535\n",
      "LightGBM Validation: 4.330466272095863\n",
      "CatBoost Validation: 8.644868063307952\n",
      "Trial: 2\n",
      "XGBoost Validation: 10.083251246830114\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1042\n",
      "[LightGBM] [Info] Number of data points in the train set: 405, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 22.712099\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LightGBM Validation: 8.455796668796213\n",
      "CatBoost Validation: 8.13000346864853\n",
      "Trial: 3\n",
      "XGBoost Validation: 12.81116460604389\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000235 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1032\n",
      "[LightGBM] [Info] Number of data points in the train set: 405, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 22.322963\n",
      "LightGBM Validation: 15.293116975972934\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CatBoost Validation: 11.749212823553451\n",
      "Trial: 4\n",
      "XGBoost Validation: 8.723749894351535\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000197 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1039\n",
      "[LightGBM] [Info] Number of data points in the train set: 405, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 22.327654\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LightGBM Validation: 9.374767798015942\n",
      "CatBoost Validation: 8.069802434283632\n",
      "Trial: 5\n",
      "XGBoost Validation: 6.08487379914099\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000202 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1041\n",
      "[LightGBM] [Info] Number of data points in the train set: 405, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 22.505432\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LightGBM Validation: 12.533191330170698\n",
      "CatBoost Validation: 7.766294065731304\n",
      "Trial: 6\n",
      "XGBoost Validation: 6.55995747666388\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1030\n",
      "[LightGBM] [Info] Number of data points in the train set: 404, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 22.796535\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LightGBM Validation: 4.330466272095863\n",
      "CatBoost Validation: 8.644868063307952\n",
      "Trial: 7\n",
      "XGBoost Validation: 10.083251246830114\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000231 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1042\n",
      "[LightGBM] [Info] Number of data points in the train set: 405, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 22.712099\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LightGBM Validation: 8.455796668796213\n",
      "CatBoost Validation: 8.13000346864853\n",
      "Trial: 8\n",
      "XGBoost Validation: 12.81116460604389\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000226 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1032\n",
      "[LightGBM] [Info] Number of data points in the train set: 405, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 22.322963\n",
      "LightGBM Validation: 15.293116975972934\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CatBoost Validation: 11.749212823553451\n",
      "Trial: 9\n",
      "XGBoost Validation: 8.723749894351535\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000205 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1039\n",
      "[LightGBM] [Info] Number of data points in the train set: 405, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 22.327654\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LightGBM Validation: 9.374767798015942\n",
      "CatBoost Validation: 8.069802434283632\n",
      "Trial: 10\n",
      "XGBoost Validation: 6.08487379914099\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000228 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1041\n",
      "[LightGBM] [Info] Number of data points in the train set: 405, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 22.505432\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LightGBM Validation: 12.533191330170698\n",
      "CatBoost Validation: 7.766294065731304\n",
      "Trial: 11\n",
      "XGBoost Validation: 6.55995747666388\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1030\n",
      "[LightGBM] [Info] Number of data points in the train set: 404, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 22.796535\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LightGBM Validation: 4.330466272095863\n",
      "CatBoost Validation: 8.644868063307952\n",
      "Trial: 12\n",
      "XGBoost Validation: 10.083251246830114\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000216 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1042\n",
      "[LightGBM] [Info] Number of data points in the train set: 405, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 22.712099\n",
      "LightGBM Validation: 8.455796668796213\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CatBoost Validation: 8.13000346864853\n",
      "Trial: 13\n",
      "XGBoost Validation: 12.81116460604389\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000225 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1032\n",
      "[LightGBM] [Info] Number of data points in the train set: 405, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 22.322963\n",
      "LightGBM Validation: 15.293116975972934\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CatBoost Validation: 11.749212823553451\n",
      "Trial: 14\n",
      "XGBoost Validation: 8.723749894351535\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000300 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1039\n",
      "[LightGBM] [Info] Number of data points in the train set: 405, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 22.327654\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LightGBM Validation: 9.374767798015942\n",
      "CatBoost Validation: 8.069802434283632\n",
      "Trial: 15\n",
      "XGBoost Validation: 6.08487379914099\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1041\n",
      "[LightGBM] [Info] Number of data points in the train set: 405, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 22.505432\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LightGBM Validation: 12.533191330170698\n",
      "CatBoost Validation: 7.766294065731304\n",
      "Trial: 16\n",
      "XGBoost Validation: 6.55995747666388\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000206 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1030\n",
      "[LightGBM] [Info] Number of data points in the train set: 404, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 22.796535\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LightGBM Validation: 4.330466272095863\n",
      "CatBoost Validation: 8.644868063307952\n",
      "Trial: 17\n",
      "XGBoost Validation: 10.083251246830114\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000202 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1042\n",
      "[LightGBM] [Info] Number of data points in the train set: 405, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 22.712099\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LightGBM Validation: 8.455796668796213\n",
      "CatBoost Validation: 8.13000346864853\n",
      "Trial: 18\n",
      "XGBoost Validation: 12.81116460604389\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000212 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1032\n",
      "[LightGBM] [Info] Number of data points in the train set: 405, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 22.322963\n",
      "LightGBM Validation: 15.293116975972934\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CatBoost Validation: 11.749212823553451\n",
      "Trial: 19\n",
      "XGBoost Validation: 8.723749894351535\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000216 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1039\n",
      "[LightGBM] [Info] Number of data points in the train set: 405, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 22.327654\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LightGBM Validation: 9.374767798015942\n",
      "CatBoost Validation: 8.069802434283632\n",
      "Trial: 20\n",
      "XGBoost Validation: 6.08487379914099\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000203 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1041\n",
      "[LightGBM] [Info] Number of data points in the train set: 405, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 22.505432\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LightGBM Validation: 12.533191330170698\n",
      "CatBoost Validation: 7.766294065731304\n",
      "Trial: 21\n",
      "XGBoost Validation: 6.55995747666388\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1030\n",
      "[LightGBM] [Info] Number of data points in the train set: 404, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 22.796535\n",
      "LightGBM Validation: 4.330466272095863\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CatBoost Validation: 8.644868063307952\n",
      "Trial: 22\n",
      "XGBoost Validation: 10.083251246830114\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000204 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1042\n",
      "[LightGBM] [Info] Number of data points in the train set: 405, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 22.712099\n",
      "LightGBM Validation: 8.455796668796213\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CatBoost Validation: 8.13000346864853\n",
      "Trial: 23\n",
      "XGBoost Validation: 12.81116460604389\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000205 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1032\n",
      "[LightGBM] [Info] Number of data points in the train set: 405, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 22.322963\n",
      "LightGBM Validation: 15.293116975972934\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CatBoost Validation: 11.749212823553451\n",
      "Trial: 24\n",
      "XGBoost Validation: 8.723749894351535\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000206 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1039\n",
      "[LightGBM] [Info] Number of data points in the train set: 405, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 22.327654\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LightGBM Validation: 9.374767798015942\n",
      "CatBoost Validation: 8.069802434283632\n",
      "Trial: 25\n",
      "XGBoost Validation: 6.08487379914099\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000209 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1041\n",
      "[LightGBM] [Info] Number of data points in the train set: 405, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 22.505432\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LightGBM Validation: 12.533191330170698\n",
      "CatBoost Validation: 7.766294065731304\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          0          1          2          3          4          5   \\\n",
       "0  27.145155  32.892010  19.417727  27.145155  32.892010  19.417727   \n",
       "1  22.010473  19.930988  14.301942  22.010473  19.930988  14.301942   \n",
       "2  27.145155  32.892010  19.417727  27.145155  32.892010  19.417727   \n",
       "3  34.725506  23.145926  15.534187  34.725506  23.145926  15.534187   \n",
       "4  33.646828  16.715572  19.566851  33.646828  16.715572  19.566851   \n",
       "\n",
       "          6          7          8          9   ...         35         36  \\\n",
       "0  27.145155  32.892010  19.417727  27.145155  ...  18.767848  26.740495   \n",
       "1  22.010473  19.930988  14.301942  22.010473  ...  13.497022  21.587954   \n",
       "2  27.145155  32.892010  19.417727  27.145155  ...  18.767848  26.740495   \n",
       "3  34.725506  23.145926  15.534187  34.725506  ...  17.575805  36.858545   \n",
       "4  33.646828  16.715572  19.566851  33.646828  ...  20.861329  33.146283   \n",
       "\n",
       "          37         38         39         40         41         42  \\\n",
       "0  35.423938  18.767848  26.740495  35.423938  18.767848  26.740495   \n",
       "1  20.173890  13.497022  21.587954  20.173890  13.497022  21.587954   \n",
       "2  35.423938  18.767848  26.740495  35.423938  18.767848  26.740495   \n",
       "3  27.144804  17.575805  36.858545  27.144804  17.575805  36.858545   \n",
       "4  17.354749  20.861329  33.146283  17.354749  20.861329  33.146283   \n",
       "\n",
       "          43         44  \n",
       "0  35.423938  18.767848  \n",
       "1  20.173890  13.497022  \n",
       "2  35.423938  18.767848  \n",
       "3  27.144804  17.575805  \n",
       "4  17.354749  20.861329  \n",
       "\n",
       "[5 rows x 45 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.145155</td>\n",
       "      <td>32.892010</td>\n",
       "      <td>19.417727</td>\n",
       "      <td>27.145155</td>\n",
       "      <td>32.892010</td>\n",
       "      <td>19.417727</td>\n",
       "      <td>27.145155</td>\n",
       "      <td>32.892010</td>\n",
       "      <td>19.417727</td>\n",
       "      <td>27.145155</td>\n",
       "      <td>...</td>\n",
       "      <td>18.767848</td>\n",
       "      <td>26.740495</td>\n",
       "      <td>35.423938</td>\n",
       "      <td>18.767848</td>\n",
       "      <td>26.740495</td>\n",
       "      <td>35.423938</td>\n",
       "      <td>18.767848</td>\n",
       "      <td>26.740495</td>\n",
       "      <td>35.423938</td>\n",
       "      <td>18.767848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.010473</td>\n",
       "      <td>19.930988</td>\n",
       "      <td>14.301942</td>\n",
       "      <td>22.010473</td>\n",
       "      <td>19.930988</td>\n",
       "      <td>14.301942</td>\n",
       "      <td>22.010473</td>\n",
       "      <td>19.930988</td>\n",
       "      <td>14.301942</td>\n",
       "      <td>22.010473</td>\n",
       "      <td>...</td>\n",
       "      <td>13.497022</td>\n",
       "      <td>21.587954</td>\n",
       "      <td>20.173890</td>\n",
       "      <td>13.497022</td>\n",
       "      <td>21.587954</td>\n",
       "      <td>20.173890</td>\n",
       "      <td>13.497022</td>\n",
       "      <td>21.587954</td>\n",
       "      <td>20.173890</td>\n",
       "      <td>13.497022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27.145155</td>\n",
       "      <td>32.892010</td>\n",
       "      <td>19.417727</td>\n",
       "      <td>27.145155</td>\n",
       "      <td>32.892010</td>\n",
       "      <td>19.417727</td>\n",
       "      <td>27.145155</td>\n",
       "      <td>32.892010</td>\n",
       "      <td>19.417727</td>\n",
       "      <td>27.145155</td>\n",
       "      <td>...</td>\n",
       "      <td>18.767848</td>\n",
       "      <td>26.740495</td>\n",
       "      <td>35.423938</td>\n",
       "      <td>18.767848</td>\n",
       "      <td>26.740495</td>\n",
       "      <td>35.423938</td>\n",
       "      <td>18.767848</td>\n",
       "      <td>26.740495</td>\n",
       "      <td>35.423938</td>\n",
       "      <td>18.767848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34.725506</td>\n",
       "      <td>23.145926</td>\n",
       "      <td>15.534187</td>\n",
       "      <td>34.725506</td>\n",
       "      <td>23.145926</td>\n",
       "      <td>15.534187</td>\n",
       "      <td>34.725506</td>\n",
       "      <td>23.145926</td>\n",
       "      <td>15.534187</td>\n",
       "      <td>34.725506</td>\n",
       "      <td>...</td>\n",
       "      <td>17.575805</td>\n",
       "      <td>36.858545</td>\n",
       "      <td>27.144804</td>\n",
       "      <td>17.575805</td>\n",
       "      <td>36.858545</td>\n",
       "      <td>27.144804</td>\n",
       "      <td>17.575805</td>\n",
       "      <td>36.858545</td>\n",
       "      <td>27.144804</td>\n",
       "      <td>17.575805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33.646828</td>\n",
       "      <td>16.715572</td>\n",
       "      <td>19.566851</td>\n",
       "      <td>33.646828</td>\n",
       "      <td>16.715572</td>\n",
       "      <td>19.566851</td>\n",
       "      <td>33.646828</td>\n",
       "      <td>16.715572</td>\n",
       "      <td>19.566851</td>\n",
       "      <td>33.646828</td>\n",
       "      <td>...</td>\n",
       "      <td>20.861329</td>\n",
       "      <td>33.146283</td>\n",
       "      <td>17.354749</td>\n",
       "      <td>20.861329</td>\n",
       "      <td>33.146283</td>\n",
       "      <td>17.354749</td>\n",
       "      <td>20.861329</td>\n",
       "      <td>33.146283</td>\n",
       "      <td>17.354749</td>\n",
       "      <td>20.861329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QXpNQN8Zs2SG",
    "outputId": "a936274b-248f-4359-c285-8f04bd36ebfc"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XGBoost で Stacking"
   ],
   "metadata": {
    "id": "kQ7_F_u0tUtu"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "loop_counts = 0\r\n",
    "\r\n",
    "# 学習データとテストデータに分ける\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(\r\n",
    "    df.drop([\"target\"], axis=1), \r\n",
    "    df[\"target\"], \r\n",
    "    random_state=42,\r\n",
    "    test_size=0.2\r\n",
    ")\r\n",
    "\r\n",
    "# 予測結果の格納用のnumpy行列を作成\r\n",
    "test_preds = np.zeros((len(y_test), 5))\r\n",
    "\r\n",
    "# 学習データの数だけの数列（0行から最終行まで連番）\r\n",
    "row_no_list = list(range(len(y_train)))\r\n",
    "\r\n",
    "# KFoldクラスをインスタンス化（これを使って5分割する）\r\n",
    "K_fold = KFold(n_splits=5, shuffle=True,  random_state=42)\r\n",
    "\r\n",
    "# KFoldクラスで分割した回数だけ実行（ここでは5回）\r\n",
    "for train_cv_no, eval_cv_no in K_fold.split(row_no_list, y_train):\r\n",
    "    # ilocで取り出す行を指定\r\n",
    "    X_train_cv = X_train.iloc[train_cv_no, :]\r\n",
    "    y_train_cv = pd.Series(y_train).iloc[train_cv_no]\r\n",
    "    X_eval_cv = X_train.iloc[eval_cv_no, :]\r\n",
    "    y_eval_cv = pd.Series(y_train).iloc[eval_cv_no]\r\n",
    "\r\n",
    "    # データを格納する\r\n",
    "    # 学習用\r\n",
    "    xgb_train = xgb.DMatrix(X_train_cv, label=y_train_cv)\r\n",
    "    # 検証用\r\n",
    "    xgb_eval = xgb.DMatrix(X_eval_cv, label=y_eval_cv)\r\n",
    "    # テスト用\r\n",
    "    xgb_test = xgb.DMatrix(X_test, label=y_test)\r\n",
    "\r\n",
    "    xgb_params = {\r\n",
    "        'objective': 'reg:squarederror',  # 回帰\r\n",
    "        'learning_rate': 0.1,           # 学習率\r\n",
    "        'eval_metric': 'rmse'           # 学習用の指標 (RMSE)\r\n",
    "    }\r\n",
    "    \r\n",
    "    # 学習\r\n",
    "    evals = [(xgb_train, 'train'), (xgb_eval, 'eval')] # 学習に用いる検証用データ\r\n",
    "    evaluation_results = {}                            # 学習の経過を保存する箱\r\n",
    "    bst = xgb.train(xgb_params,                        # 上記で設定したパラメータ\r\n",
    "                    xgb_train,                         # 使用するデータセット\r\n",
    "                    num_boost_round=200,               # 学習の回数\r\n",
    "                    early_stopping_rounds=10,          # アーリーストッピング\r\n",
    "                    evals=evals,                       # 学習経過で表示する名称\r\n",
    "                    evals_result=evaluation_results,   # 上記で設定した検証用データ\r\n",
    "                    verbose_eval=0                     # 学習の経過の表示(非表示)\r\n",
    "                    )\r\n",
    "\r\n",
    "    y_pred = bst.predict(xgb_test, ntree_limit=bst.best_ntree_limit)\r\n",
    "    \r\n",
    "    # testの予測を保存\r\n",
    "    test_preds[:, loop_counts] = y_pred\r\n",
    " \r\n",
    "    print('Trial: ' + str(loop_counts))\r\n",
    "    loop_counts += 1    \r\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=True)\r\n",
    "    \r\n",
    "    print('RMSE:', rmse)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Trial: 0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RMSE: 10.12527605234543\n",
      "Trial: 1\n",
      "RMSE: 6.620539824211737\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Trial: 2\n",
      "RMSE: 10.36307283523485\n",
      "Trial: 3\n",
      "RMSE: 9.720079862500858\n",
      "Trial: 4\n",
      "RMSE: 5.15608715971082\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kojis\\miniconda3\\envs\\pip38\\lib\\site-packages\\xgboost\\core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ULQkepfFtU61",
    "outputId": "83809682-00c3-4fba-909d-0386d3f90f26"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 予測"
   ],
   "metadata": {
    "id": "TCdg1JlNuE2q"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "y_pred_mean = test_preds.mean(axis=1)\r\n",
    "\r\n",
    "mean_squared_error(y_test, y_pred_mean, squared=True)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7.002718516727869"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B7rmipRttg24",
    "outputId": "0fb5a727-0afd-4348-9d4a-96dd40e92b47"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# 係数は適当にいじってください\r\n",
    "a, b, c, d, e = 0.1, 0.2, 0,1, 0.1, 0.6\r\n",
    "\r\n",
    "y_pred = test_preds[:, 0] * a + test_preds[:, 1] * b + test_preds[:, 2] * c + test_preds[:, 3] * d + test_preds[:, 4] * e"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9.036437175317653"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-gpqYloauUG2",
    "outputId": "57c29978-bbdb-4492-feec-6302947e7ebd"
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Stacking.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}